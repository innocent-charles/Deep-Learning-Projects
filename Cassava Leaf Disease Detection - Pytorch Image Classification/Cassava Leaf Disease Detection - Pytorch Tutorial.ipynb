{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Import Modules","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport time\nimport os\nimport copy\nimport json\n\n# visualization modules\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom PIL import Image\n\n# pytorch modules\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.nn import functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import models\nimport torchvision.transforms as transforms\n\n# augmentation\nimport albumentations\nfrom albumentations.pytorch.transforms import ToTensorV2\n\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2022-01-18T03:22:49.208883Z","iopub.execute_input":"2022-01-18T03:22:49.209526Z","iopub.status.idle":"2022-01-18T03:22:49.220825Z","shell.execute_reply.started":"2022-01-18T03:22:49.209451Z","shell.execute_reply":"2022-01-18T03:22:49.219902Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"## Load the Dataset","metadata":{}},{"cell_type":"code","source":"BASE_DIR = \"../input/cassava-leaf-disease-classification/\"\n\ntrain = pd.read_csv(BASE_DIR+'train.csv')\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-18T03:22:49.222421Z","iopub.execute_input":"2022-01-18T03:22:49.222699Z","iopub.status.idle":"2022-01-18T03:22:49.254069Z","shell.execute_reply.started":"2022-01-18T03:22:49.222665Z","shell.execute_reply":"2022-01-18T03:22:49.253395Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"# loading mapping for target label\nwith open(BASE_DIR+'label_num_to_disease_map.json') as f:\n    mapping = json.loads(f.read())\n    mapping = {int(k): v for k, v in mapping.items()}\nmapping","metadata":{"execution":{"iopub.status.busy":"2022-01-18T03:22:49.255341Z","iopub.execute_input":"2022-01-18T03:22:49.256113Z","iopub.status.idle":"2022-01-18T03:22:49.265063Z","shell.execute_reply.started":"2022-01-18T03:22:49.256073Z","shell.execute_reply":"2022-01-18T03:22:49.264052Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"train['label_names'] = train['label'].map(mapping)\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-18T03:22:49.266639Z","iopub.execute_input":"2022-01-18T03:22:49.267029Z","iopub.status.idle":"2022-01-18T03:22:49.280091Z","shell.execute_reply.started":"2022-01-18T03:22:49.266988Z","shell.execute_reply":"2022-01-18T03:22:49.279309Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"## Exploratory Data Analysis","metadata":{}},{"cell_type":"code","source":"def plot_images(class_id, label, total_images=6):\n    # get image ids corresponding to the target class id\n    plot_list = train[train['label']==class_id].sample(total_images)['image_id'].tolist()\n    \n    labels = [label for i in range(total_images)]\n    # dynamically set size for subplot\n    size = int(np.sqrt(total_images))\n    if size*size < total_images:\n        size += 1\n    \n    # set figure size\n    plt.figure(figsize=(15,15))\n    \n    # plot the image in subplot\n    for index, (image_id, label) in enumerate(zip(plot_list, labels)):\n        plt.subplot(size, size, index+1)\n        image = Image.open(str(BASE_DIR+'train_images/'+image_id))\n        plt.imshow(image)\n        plt.title(label, fontsize=14)\n        plt.axis('off')\n        \n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-18T03:22:49.281393Z","iopub.execute_input":"2022-01-18T03:22:49.281864Z","iopub.status.idle":"2022-01-18T03:22:49.290008Z","shell.execute_reply.started":"2022-01-18T03:22:49.281827Z","shell.execute_reply":"2022-01-18T03:22:49.289329Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"plot_images(0, mapping[0], 6)","metadata":{"execution":{"iopub.status.busy":"2022-01-18T03:22:49.291401Z","iopub.execute_input":"2022-01-18T03:22:49.291889Z","iopub.status.idle":"2022-01-18T03:22:50.091827Z","shell.execute_reply.started":"2022-01-18T03:22:49.291850Z","shell.execute_reply":"2022-01-18T03:22:50.090996Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"plot_images(1, mapping[1], 6)","metadata":{"execution":{"iopub.status.busy":"2022-01-18T03:22:50.092929Z","iopub.execute_input":"2022-01-18T03:22:50.093331Z","iopub.status.idle":"2022-01-18T03:22:51.071311Z","shell.execute_reply.started":"2022-01-18T03:22:50.093297Z","shell.execute_reply":"2022-01-18T03:22:51.070715Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"plot_images(2, mapping[2], 6)","metadata":{"execution":{"iopub.status.busy":"2022-01-18T03:22:51.072340Z","iopub.execute_input":"2022-01-18T03:22:51.072876Z","iopub.status.idle":"2022-01-18T03:22:51.864045Z","shell.execute_reply.started":"2022-01-18T03:22:51.072835Z","shell.execute_reply":"2022-01-18T03:22:51.863344Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"plot_images(3, mapping[3], 6)","metadata":{"execution":{"iopub.status.busy":"2022-01-18T03:22:51.865215Z","iopub.execute_input":"2022-01-18T03:22:51.865712Z","iopub.status.idle":"2022-01-18T03:22:52.673838Z","shell.execute_reply.started":"2022-01-18T03:22:51.865673Z","shell.execute_reply":"2022-01-18T03:22:52.672326Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"plot_images(4, mapping[4], 6)","metadata":{"execution":{"iopub.status.busy":"2022-01-18T03:22:52.674954Z","iopub.execute_input":"2022-01-18T03:22:52.675651Z","iopub.status.idle":"2022-01-18T03:22:53.485152Z","shell.execute_reply.started":"2022-01-18T03:22:52.675613Z","shell.execute_reply":"2022-01-18T03:22:53.484452Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"# class distribution\nsns.countplot(train['label_names'])\nplt.xticks(rotation=90)","metadata":{"execution":{"iopub.status.busy":"2022-01-18T03:22:53.486244Z","iopub.execute_input":"2022-01-18T03:22:53.486618Z","iopub.status.idle":"2022-01-18T03:22:53.786764Z","shell.execute_reply.started":"2022-01-18T03:22:53.486583Z","shell.execute_reply":"2022-01-18T03:22:53.786074Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"## Configuration and Utility Functions","metadata":{}},{"cell_type":"code","source":"DIM = (256, 256)\nWIDTH, HEIGHT = DIM\nNUM_CLASSES = 5\nNUM_WORKERS = 24\nTRAIN_BATCH_SIZE = 32\nTEST_BATCH_SIZE = 32\nSEED = 1\n\nDEVICE = 'cuda'\n\nMEAN = [0.485, 0.456, 0.406]\nSTD = [0.229, 0.224, 0.225]","metadata":{"execution":{"iopub.status.busy":"2022-01-18T03:22:53.790536Z","iopub.execute_input":"2022-01-18T03:22:53.792432Z","iopub.status.idle":"2022-01-18T03:22:53.802028Z","shell.execute_reply.started":"2022-01-18T03:22:53.792393Z","shell.execute_reply":"2022-01-18T03:22:53.798229Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"## Augmentations","metadata":{}},{"cell_type":"code","source":"def get_test_transforms(value = 'val'):\n    if value == 'train':\n        return albumentations.Compose([\n            albumentations.Resize(WIDTH, HEIGHT),\n            albumentations.HorizontalFlip(p=0.5),\n            albumentations.Rotate(limit=(-90, 90)),\n            albumentations.VerticalFlip(p=0.5),\n            albumentations.Normalize(MEAN, STD, max_pixel_value=255.0, always_apply=True),\n            ToTensorV2(p=1.0)\n        ])\n    elif value == 'val':\n        return albumentations.Compose([\n            albumentations.Resize(WIDTH, HEIGHT),\n            albumentations.Normalize(MEAN, STD, max_pixel_value=255.0, always_apply=True),\n            ToTensorV2(p=1.0)\n        ])","metadata":{"execution":{"iopub.status.busy":"2022-01-18T03:22:53.803448Z","iopub.execute_input":"2022-01-18T03:22:53.803947Z","iopub.status.idle":"2022-01-18T03:22:53.817295Z","shell.execute_reply.started":"2022-01-18T03:22:53.803910Z","shell.execute_reply":"2022-01-18T03:22:53.816458Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"## Dataset Loader Class","metadata":{}},{"cell_type":"code","source":"class CassavaDataset(Dataset):\n    def __init__(self, image_ids, labels, dimension=None, augmentations=None, folder='train_images'):\n        super().__init__()\n        self.image_ids = image_ids\n        self.labels = labels\n        self.dim = dimension\n        self.augmentations = augmentations\n        self.folder = folder\n    \n    # returns the length\n    def __len__(self):\n        return len(self.image_ids)\n    \n    # return the image and label for that index\n    def __getitem__(self, idx):\n        img = Image.open(os.path.join(BASE_DIR, self.folder, self.image_ids[idx]))\n        \n        if self.dim:\n            img = img.resize(self.dim)\n        \n        # convert to numpy array\n        img = np.array(img)\n        \n        if self.augmentations:\n            augmented = self.augmentations(image=img)\n            img = augmented['image']\n        \n        label = torch.tensor(self.labels[idx], dtype=torch.long)\n        return img, label","metadata":{"execution":{"iopub.status.busy":"2022-01-18T03:22:53.818782Z","iopub.execute_input":"2022-01-18T03:22:53.819148Z","iopub.status.idle":"2022-01-18T03:22:53.833440Z","shell.execute_reply.started":"2022-01-18T03:22:53.819115Z","shell.execute_reply":"2022-01-18T03:22:53.832545Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":"## Train Test Split","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(train['image_id'], train['label'], test_size=0.25)","metadata":{"execution":{"iopub.status.busy":"2022-01-18T03:22:53.834833Z","iopub.execute_input":"2022-01-18T03:22:53.836648Z","iopub.status.idle":"2022-01-18T03:22:53.850990Z","shell.execute_reply.started":"2022-01-18T03:22:53.836609Z","shell.execute_reply":"2022-01-18T03:22:53.850221Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import WeightedRandomSampler\ndef sampler_(labels):\n    label_unique, counts = np.unique(labels, return_counts=True)\n    print('Unique Labels', label_unique)\n    weights = [sum(counts) / c for c in counts]\n    sample_weights = [weights[w] for w in labels]\n    sampler = WeightedRandomSampler(sample_weights, len(sample_weights), replacement=True)\n    return sampler","metadata":{"execution":{"iopub.status.busy":"2022-01-18T03:22:53.852289Z","iopub.execute_input":"2022-01-18T03:22:53.852799Z","iopub.status.idle":"2022-01-18T03:22:53.867855Z","shell.execute_reply.started":"2022-01-18T03:22:53.852759Z","shell.execute_reply":"2022-01-18T03:22:53.866387Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"train_sampler = sampler_(y_train)","metadata":{"execution":{"iopub.status.busy":"2022-01-18T03:22:53.869408Z","iopub.execute_input":"2022-01-18T03:22:53.869900Z","iopub.status.idle":"2022-01-18T03:22:53.893541Z","shell.execute_reply.started":"2022-01-18T03:22:53.869860Z","shell.execute_reply":"2022-01-18T03:22:53.892888Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"# create dataloaders for training antrain_test_splitidation\ntrain_dataset = CassavaDataset(\n    image_ids=x_train.values,\n    labels=y_train.values,\n    augmentations=get_test_transforms('train'),\n    dimension=DIM\n)\n\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=TRAIN_BATCH_SIZE,\n    num_workers=NUM_WORKERS,\n    shuffle=False,\n    sampler=train_sampler\n)\n\nval_dataset = CassavaDataset(\n    image_ids=x_test.values,\n    labels=y_test.values,\n    augmentations=get_test_transforms('val'),\n    dimension=DIM\n)\n\nval_loader = DataLoader(\n    val_dataset,\n    batch_size=TRAIN_BATCH_SIZE,\n    num_workers=NUM_WORKERS,\n    shuffle=False\n)\n\nloaders = {'train': train_loader, 'val': val_loader}","metadata":{"execution":{"iopub.status.busy":"2022-01-18T03:22:53.894723Z","iopub.execute_input":"2022-01-18T03:22:53.898430Z","iopub.status.idle":"2022-01-18T03:22:53.908041Z","shell.execute_reply.started":"2022-01-18T03:22:53.898394Z","shell.execute_reply":"2022-01-18T03:22:53.907241Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"# to check whether dataset is working or not\n# fetch the data based on index\nval_dataset[0]","metadata":{"execution":{"iopub.status.busy":"2022-01-18T03:22:53.911692Z","iopub.execute_input":"2022-01-18T03:22:53.912315Z","iopub.status.idle":"2022-01-18T03:22:53.958179Z","shell.execute_reply.started":"2022-01-18T03:22:53.912278Z","shell.execute_reply":"2022-01-18T03:22:53.957317Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"markdown","source":"## Use Pretrained Model (Transfer Learning)","metadata":{}},{"cell_type":"code","source":"def getModel():\n    net = models.resnet152(pretrained=True)\n    \n    # if you want to train the whole network, comment this code\n    # freeze all the layers in the network\n    for param in net.parameters():\n        param.requires_grad = False\n        \n    num_ftrs = net.fc.in_features\n    # create last few layers\n    net.fc = nn.Sequential(\n        nn.Linear(num_ftrs, 256),\n        nn.ReLU(),\n        nn.Dropout(0.3),\n        nn.Linear(256, NUM_CLASSES),\n        nn.LogSoftmax(dim=1)\n    )\n    \n    # use gpu if any\n    net = net.cuda() if DEVICE else net\n    return net","metadata":{"execution":{"iopub.status.busy":"2022-01-18T03:22:53.966264Z","iopub.execute_input":"2022-01-18T03:22:53.968270Z","iopub.status.idle":"2022-01-18T03:22:53.977301Z","shell.execute_reply.started":"2022-01-18T03:22:53.968228Z","shell.execute_reply":"2022-01-18T03:22:53.976535Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"model = getModel()","metadata":{"execution":{"iopub.status.busy":"2022-01-18T03:23:41.545657Z","iopub.execute_input":"2022-01-18T03:23:41.546375Z","iopub.status.idle":"2022-01-18T03:23:47.186126Z","shell.execute_reply.started":"2022-01-18T03:23:41.546328Z","shell.execute_reply":"2022-01-18T03:23:47.185376Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"import math\ndef cyclical_lr(stepsize, min_lr=3e-4, max_lr=3e-3):\n\n    # Scaler: we can adapt this if we do not want the triangular CLR\n    scaler = lambda x: 1.\n\n    # Lambda function to calculate the LR\n    lr_lambda = lambda it: min_lr + (max_lr - min_lr) * relative(it, stepsize)\n\n    # Additional function to see where on the cycle we are\n    def relative(it, stepsize):\n        cycle = math.floor(1 + it / (2 * stepsize))\n        x = abs(it / stepsize - 2 * cycle + 1)\n        return max(0, (1 - x)) * scaler(cycle)\n\n    return lr_lambda","metadata":{"execution":{"iopub.status.busy":"2022-01-18T03:23:48.334546Z","iopub.execute_input":"2022-01-18T03:23:48.334996Z","iopub.status.idle":"2022-01-18T03:23:48.340920Z","shell.execute_reply.started":"2022-01-18T03:23:48.334964Z","shell.execute_reply":"2022-01-18T03:23:48.340044Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()\n# optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\noptimizer = torch.optim.SGD(model.parameters(), lr=1., momentum=0.9)\nstep_size = 4*len(train_loader)\nclr = cyclical_lr(step_size, min_lr=3e-4, max_lr=3e-3)\nscheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, [clr])","metadata":{"execution":{"iopub.status.busy":"2022-01-18T03:23:48.663581Z","iopub.execute_input":"2022-01-18T03:23:48.664345Z","iopub.status.idle":"2022-01-18T03:23:48.673069Z","shell.execute_reply.started":"2022-01-18T03:23:48.664310Z","shell.execute_reply":"2022-01-18T03:23:48.672125Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"# print(model)","metadata":{"execution":{"iopub.status.busy":"2022-01-18T03:23:49.544109Z","iopub.execute_input":"2022-01-18T03:23:49.544389Z","iopub.status.idle":"2022-01-18T03:23:49.549579Z","shell.execute_reply.started":"2022-01-18T03:23:49.544357Z","shell.execute_reply":"2022-01-18T03:23:49.547202Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"# freeze (or) unfreeze all the layers\nunfreeze = True # to freeze, set it as False\nfor param in model.parameters():\n    param.requires_grad = unfreeze","metadata":{"execution":{"iopub.status.busy":"2022-01-18T03:24:19.836207Z","iopub.execute_input":"2022-01-18T03:24:19.836479Z","iopub.status.idle":"2022-01-18T03:24:19.844975Z","shell.execute_reply.started":"2022-01-18T03:24:19.836448Z","shell.execute_reply":"2022-01-18T03:24:19.844184Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"# find total parameters and trainable parameters\ntotal_params = sum(p.numel() for p in model.parameters())\nprint(f'{total_params:,} total parameters')\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(f'{trainable_params:,} training parameters')","metadata":{"execution":{"iopub.status.busy":"2022-01-18T03:24:20.562236Z","iopub.execute_input":"2022-01-18T03:24:20.562798Z","iopub.status.idle":"2022-01-18T03:24:20.573025Z","shell.execute_reply.started":"2022-01-18T03:24:20.562759Z","shell.execute_reply":"2022-01-18T03:24:20.572228Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"markdown","source":"## Steps for Training and Validation","metadata":{}},{"cell_type":"code","source":"def train_model(model, dataloaders, criterion, optimizer, num_epochs=5, scheduler=scheduler):\n    # set starting time\n    start_time = time.time()\n    \n    val_acc_history = []\n    \n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_acc = 0.0\n    \n    for epoch in range(num_epochs):\n        print(f'Epoch {epoch}/{num_epochs-1}')\n        print('-'*15)\n        \n        # each epoch have training and validation phase\n        for phase in ['train', 'val']:\n            # set mode for model\n            if phase == 'train':\n                model.train() # set model to training mode\n            else:\n                model.eval() # set model to evaluate mode\n                \n            running_loss = 0.0\n            running_corrects = 0\n            fin_out = []\n            \n            # iterate over data\n            for inputs, labels in dataloaders[phase]:\n                # move data to corresponding hardware\n                inputs = inputs.to(DEVICE)\n                labels = labels.to(DEVICE)\n                \n                # reset (or) zero the parameter gradients\n                optimizer.zero_grad()\n                \n                # training (or) validation process\n                with torch.set_grad_enabled(phase=='train'):\n                    outputs = model(inputs)\n                    loss = criterion(outputs, labels)\n                    \n                    _, preds = torch.max(outputs, 1)\n                    \n                    # back propagation in the network\n                    if phase == 'train':\n                        loss.backward()\n                        optimizer.step()\n                        scheduler.step()\n                        \n                running_loss += loss.item() * inputs.size(0)\n                running_corrects += torch.sum(preds == labels.data)\n                \n            # calculate loss and accuarcy for the epoch\n            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n            \n            # print loss and acc for training & validation\n            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n            \n            # update the best weights\n            if phase == 'val' and epoch_acc > best_acc:\n                best_acc = epoch_acc\n                best_model_wts = copy.deepcopy(model.state_dict())\n            if phase == 'val':\n                val_acc_history.append(epoch_acc)\n                \n        print()\n    end_time = time.time() - start_time\n    \n    print('Training completes in {:.0f}m {:.0f}s'.format(end_time // 60, end_time % 60))\n    print('Best Val Acc: {:.4f}'.format(best_acc))\n    \n    # load best model weights\n    model.load_state_dict(best_model_wts)\n    return model, val_acc_history","metadata":{"execution":{"iopub.status.busy":"2022-01-18T03:25:28.120529Z","iopub.execute_input":"2022-01-18T03:25:28.120961Z","iopub.status.idle":"2022-01-18T03:25:28.137293Z","shell.execute_reply.started":"2022-01-18T03:25:28.120921Z","shell.execute_reply":"2022-01-18T03:25:28.136533Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"# train the model\nmodel, accuracy = train_model(model=model, dataloaders=loaders, criterion=criterion, optimizer=optimizer, num_epochs=5, scheduler=scheduler)","metadata":{"execution":{"iopub.status.busy":"2022-01-18T03:25:28.648543Z","iopub.execute_input":"2022-01-18T03:25:28.649067Z","iopub.status.idle":"2022-01-18T03:56:23.802993Z","shell.execute_reply.started":"2022-01-18T03:25:28.649033Z","shell.execute_reply":"2022-01-18T03:56:23.801475Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"# save the model and model weights\ntorch.save(model, '/kaggle/working/best_model.h5')\ntorch.save(model.state_dict(), '/kaggle/working/best_model_weights')","metadata":{"execution":{"iopub.status.busy":"2022-01-01T14:12:27.165154Z","iopub.execute_input":"2022-01-01T14:12:27.165516Z","iopub.status.idle":"2022-01-01T14:12:28.163537Z","shell.execute_reply.started":"2022-01-01T14:12:27.16548Z","shell.execute_reply":"2022-01-01T14:12:28.162402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# freeze (or) unfreeze all the layers\nunfreeze = True # to freeze, set it as False\nfor param in model.parameters():\n    param.requires_grad = unfreeze","metadata":{"execution":{"iopub.status.busy":"2022-01-01T15:50:56.349245Z","iopub.execute_input":"2022-01-01T15:50:56.349914Z","iopub.status.idle":"2022-01-01T15:50:56.360381Z","shell.execute_reply.started":"2022-01-01T15:50:56.349875Z","shell.execute_reply":"2022-01-01T15:50:56.359292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # unfreeze seleected layers\n# layers = list(range(5,7))\n# i = 0\n# for layer in model.children():\n#     if i in layers:\n#         for param in layer.parameters():\n#             param.requires_grad = True\n#     i += 1","metadata":{"execution":{"iopub.status.busy":"2022-01-01T14:53:29.042309Z","iopub.execute_input":"2022-01-01T14:53:29.042946Z","iopub.status.idle":"2022-01-01T14:53:29.053886Z","shell.execute_reply.started":"2022-01-01T14:53:29.042908Z","shell.execute_reply":"2022-01-01T14:53:29.04993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# find total parameters and trainable parameters\ntotal_params = sum(p.numel() for p in model.parameters())\nprint(f'{total_params:,} total parameters')\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(f'{trainable_params:,} training parameters')","metadata":{"execution":{"iopub.status.busy":"2022-01-01T15:26:36.514006Z","iopub.execute_input":"2022-01-01T15:26:36.514313Z","iopub.status.idle":"2022-01-01T15:26:36.527036Z","shell.execute_reply.started":"2022-01-01T15:26:36.514279Z","shell.execute_reply":"2022-01-01T15:26:36.525698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # gives the number of layers\n# for i,layer in enumerate(model.children()):\n#     print(i)","metadata":{"execution":{"iopub.status.busy":"2022-01-01T15:26:39.983821Z","iopub.execute_input":"2022-01-01T15:26:39.984432Z","iopub.status.idle":"2022-01-01T15:26:39.988823Z","shell.execute_reply.started":"2022-01-01T15:26:39.984394Z","shell.execute_reply":"2022-01-01T15:26:39.987547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Testing the Model","metadata":{}},{"cell_type":"code","source":"# empty the cache from cuda device\ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2022-01-01T14:18:11.427158Z","iopub.execute_input":"2022-01-01T14:18:11.427784Z","iopub.status.idle":"2022-01-01T14:18:11.453573Z","shell.execute_reply.started":"2022-01-01T14:18:11.42775Z","shell.execute_reply":"2022-01-01T14:18:11.452614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict(model, dataloader, device):\n    # set mode to eval\n    model.eval()\n    fin_out = []\n    \n    with torch.no_grad():\n        for images, targets in dataloader:\n            images = images.to(device)\n            targets = targets.to(device)\n            \n            outputs = model(images)\n            \n            fin_out.append(F.softmax(outputs, dim=1).detach().cpu().numpy())\n            \n    return np.concatenate(fin_out)","metadata":{"execution":{"iopub.status.busy":"2022-01-01T14:18:58.06323Z","iopub.execute_input":"2022-01-01T14:18:58.064Z","iopub.status.idle":"2022-01-01T14:18:58.071037Z","shell.execute_reply.started":"2022-01-01T14:18:58.063932Z","shell.execute_reply":"2022-01-01T14:18:58.069879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# steps for model prediction\ndevice = torch.device('cuda') # if you don't have gpu, set it as cpu\nmodel.to(device)\npred = predict(model, val_loader, device)\npred = pred.argmax(axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-01-01T14:18:58.175784Z","iopub.execute_input":"2022-01-01T14:18:58.176243Z","iopub.status.idle":"2022-01-01T14:20:26.890819Z","shell.execute_reply.started":"2022-01-01T14:18:58.176187Z","shell.execute_reply":"2022-01-01T14:20:26.889679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test.values[:5]","metadata":{"execution":{"iopub.status.busy":"2022-01-01T14:23:20.453599Z","iopub.execute_input":"2022-01-01T14:23:20.454514Z","iopub.status.idle":"2022-01-01T14:23:20.465378Z","shell.execute_reply.started":"2022-01-01T14:23:20.454477Z","shell.execute_reply":"2022-01-01T14:23:20.464272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred[:5]","metadata":{"execution":{"iopub.status.busy":"2022-01-01T14:22:53.637553Z","iopub.execute_input":"2022-01-01T14:22:53.638124Z","iopub.status.idle":"2022-01-01T14:22:53.654611Z","shell.execute_reply.started":"2022-01-01T14:22:53.638049Z","shell.execute_reply":"2022-01-01T14:22:53.653165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}